{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdiXNcQHZNEdfJNJdJakzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sebastian352/Endoscope-Semantic-Segmentation-using-Unet/blob/master/unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Project')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3FIiEQ8x_Ku",
        "outputId": "9158d3d9-7ba4-4504-9c81-f0c3343a3b43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/Project; to attempt to forcibly remount, call drive.mount(\"/content/Project\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/Project/MyDrive/Project/video_archive.zip -d /content/sample_data/video_content"
      ],
      "metadata": {
        "id": "L4bZ2zP_x_-f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M4vKyjoN3YGH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau,CSVLogger\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(input, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "s5xl_fo44QJa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_block(input, num_filters):\n",
        "    x = conv_block(input, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p"
      ],
      "metadata": {
        "id": "lapYYyULR_vG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_block(input, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x"
      ],
      "metadata": {
        "id": "h5VHtGrj9CQR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024)\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "t6NADJ0g_4-O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir(path):\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ],
      "metadata": {
        "id": "bJC-C4mOR5Tq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def move_data(path,rawImagesPath,maskImagesPath):\n",
        "  for directory in os.listdir(path):\n",
        "    dirPath = os.path.join(path, directory)\n",
        "    for subDir in os.listdir(dirPath):\n",
        "      subSubDir = os.path.join(dirPath, subDir)\n",
        "      for image in os.listdir(subSubDir):\n",
        "        srcPath = os.path.join(subSubDir, image)\n",
        "        newName = subDir + image\n",
        "        if 'mask' not in image:\n",
        "          newPath = os.path.join(rawImagesPath, newName)\n",
        "          destPath = os.path.join(rawImagesPath, image)\n",
        "          shutil.copy(srcPath, destPath)\n",
        "          os.rename(destPath, newPath)\n",
        "        if 'color' in image:\n",
        "          newPath = os.path.join(maskImagesPath, newName)\n",
        "          destPath = os.path.join(maskImagesPath, image)\n",
        "          shutil.copy(srcPath, destPath)\n",
        "          os.rename(destPath, newPath)"
      ],
      "metadata": {
        "id": "f1dvvBuMyaRI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_path(images):\n",
        "  for i in range(len(images)):\n",
        "    if 'mask' not in images[i]:\n",
        "      images[i] = os.path.join(rawImagesPath,images[i])\n",
        "    else:\n",
        "      images[i] = os.path.join(maskImagesPath,images[i])\n",
        "  return images"
      ],
      "metadata": {
        "id": "Lcy9q-vTNBN1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Unable to read image from path: {image_path}\")\n",
        "\n",
        "    img = cv2.resize(img, (IMG_W, IMG_H))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def read_mask(mask_path):\n",
        "  img = cv2.imread(mask_path,cv2.IMREAD_COLOR)\n",
        "\n",
        "  if img is None:\n",
        "    raise ValueError(f\"Unable to read image from path: {mask_path}\")\n",
        "\n",
        "  img = cv2.resize(img,(IMG_W,IMG_H))\n",
        "  cv2_imshow(img)\n",
        "  output = []\n",
        "  for color,_ in color_class_mapping.items():\n",
        "    cmap = np.all(np.equal(img,color),axis=-1)\n",
        "    output.append(cmap)\n",
        "\n",
        "  output = np.stack(output,axis=-1)\n",
        "  output = output.astype(np.uint8)\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "ZWFjpacl5dxB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_path,mask_path):\n",
        "  def read(image_path,mask_path):\n",
        "    image_path = image_path.np.decode()\n",
        "    mask_path = mask_path.np.decode()\n",
        "\n",
        "    x = read_image(image_path)\n",
        "    y = read_mask(mask_path)\n",
        "    return x,y\n",
        "\n",
        "  image,mask = tf.numpy_function(read,[image_path,mask_path],[tf.float32,tf.uint8])\n",
        "  image.set_shape([IMG_H,IMG_W,3])\n",
        "  mask.set_shape([IMG_H,IMG_W,NUM_CLASSES])\n",
        "\n",
        "  # Add a batch dimension to the input data\n",
        "  image = tf.expand_dims(image, axis=0)\n",
        "  mask = tf.expand_dims(mask, axis=0)\n",
        "\n",
        "  return image,mask"
      ],
      "metadata": {
        "id": "BPMJ4-9x9zpv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_dataset(image_path,mask_path,batch=8):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((image_path,mask_path))\n",
        "  dataset = dataset.shuffle(buffer_size = 5000)\n",
        "  dataset = dataset.map(preprocess)\n",
        "  dataset = dataset.prefetch(2)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "TlVWxZX__1vn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(images_raw,images_mask):\n",
        "  images = []\n",
        "  masks = []\n",
        "  for i in range(len(images_raw)):\n",
        "    image = read_image(images_raw[i])\n",
        "    mask = read_image(images_mask[i])\n",
        "\n",
        "    # Add a batch dimension to the input data\n",
        "    images.append(image)\n",
        "    masks.append(mask)\n",
        "  return images,masks"
      ],
      "metadata": {
        "id": "wMqCX5kManDK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "Zk3fv01_wYsK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Hyperparameters \"\"\"\n",
        "IMG_H = 320\n",
        "IMG_W = 416\n",
        "NUM_CLASSES = 12\n",
        "input_shape = (IMG_H,IMG_W, 3)\n",
        "batch_size = 32\n",
        "lr = 1e-4\n",
        "epochs = 100\n",
        "\n",
        "path = '/content/sample_data/video_content'\n",
        "opPath = '/content'\n",
        "\n",
        "model_path = os.path.join(opPath,\"model.h5\")\n",
        "csv_path = os.path.join(opPath,\"data.csv\")\n",
        "\n",
        "\n",
        "rawImagesPath = os.path.join(opPath, 'rawImages')\n",
        "maskImagesPath = os.path.join(opPath, 'maskImages')\n",
        "labelsPath = os.path.join(opPath, 'labels')\n",
        "\n",
        "imgTrainPath = os.path.join(opPath,'images','train')\n",
        "imgCrossPath=os.path.join(opPath,'images','cross')\n",
        "imgTestPath=os.path.join(opPath,'images','test')\n",
        "\n",
        "labelTrainPath=os.path.join(opPath,'labels','train')\n",
        "labelCrossPath=os.path.join(opPath,'labels','cross')\n",
        "labelTestPath=os.path.join(opPath,'labels','test')\n"
      ],
      "metadata": {
        "id": "FvH6SZXgwfay"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Loading the dataset \"\"\"\n",
        "\n",
        "create_dir(rawImagesPath)\n",
        "create_dir(maskImagesPath)\n",
        "# create_dir(labelsPath)\n",
        "\n",
        "# create_dir(imgTrainPath)\n",
        "# create_dir(imgCrossPath)\n",
        "# create_dir(imgTestPath)\n",
        "\n",
        "# create_dir(labelTrainPath)\n",
        "# create_dir(labelCrossPath)\n",
        "# create_dir(labelTestPath)\n",
        "\n",
        "# move_data(path,rawImagesPath,maskImagesPath)"
      ],
      "metadata": {
        "id": "VyqOYXWcxeWX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawImages = sorted(os.listdir(rawImagesPath))\n",
        "maskImages = sorted(os.listdir(maskImagesPath))\n",
        "add_path(rawImages)\n",
        "add_path(maskImages)"
      ],
      "metadata": {
        "id": "E1ZcEWX8M24d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Splitting data \"\"\"\n",
        "train_x, temp_x, train_y,temp_y = train_test_split(rawImages,maskImages,test_size=0.3,random_state=42)\n",
        "val_x,test_x ,val_y, test_y = train_test_split(temp_x,temp_y,test_size=0.5,random_state=42)\n"
      ],
      "metadata": {
        "id": "hDWIyUFSLpXL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x,train_y = load_images(train_x,train_y)"
      ],
      "metadata": {
        "id": "WP8edrPgd_Yc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)"
      ],
      "metadata": {
        "id": "-ODAvhQieB7_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelEncoder = LabelEncoder()\n",
        "\n",
        "# n,h,w,c = train_x.shape\n",
        "train_x = train_x.reshape(-1,1)\n",
        "train_x = labelEncoder.fit_transform(train_x)\n",
        "train_x = train_x.reshape(n,h,w,c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y80lYM9SeMyr",
        "outputId": "581ea1e6-290d-48ca-a335-1f669b155b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Process the color map \"\"\"\n",
        "color_class_mapping={(127, 127, 127): 0,\n",
        "                    (140, 140, 210): 1,\n",
        "                    (114, 114, 255): 2,\n",
        "                    (156, 70, 231): 3,\n",
        "                    (75, 183, 186): 4,\n",
        "                    (0, 255, 170): 5,\n",
        "                    (0, 85, 255): 6,\n",
        "                    (0, 0, 255): 7,\n",
        "                    (0, 255, 255): 8,\n",
        "                    (184, 255, 169): 9,\n",
        "                    (165, 160, 255): 10,\n",
        "                    (128, 50, 0): 11,\n",
        "                    (0, 74, 111): 12}\n",
        "\n",
        "classNameMapping = {\n",
        "    0: 'Black Background',\n",
        "    1: 'Abdominal Wall',\n",
        "    2: 'Liver',\n",
        "    3: 'Gastrointestinal Tract',\n",
        "    4: 'Fat',\n",
        "    5: 'Grasper',\n",
        "    6: 'Connective Tissue',\n",
        "    7: 'Blood',\n",
        "    8: 'Cystic Duct',\n",
        "    9: 'L-hook Electrocautery',\n",
        "    10: 'Gallbladder',\n",
        "    11: 'Hepatic Vein',\n",
        "    12: 'Liver Ligament'\n",
        "}"
      ],
      "metadata": {
        "id": "EhstpK8e2X9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Dataset Pipeline \"\"\"\n",
        "\n",
        "train_dataset = tf_dataset(train_x,train_y,batch = batch_size)\n",
        "valid_dataset = tf_dataset(val_x,val_y,batch=batch_size)\n"
      ],
      "metadata": {
        "id": "BPATrkKiMj49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_images(rawImages,maskImages)"
      ],
      "metadata": {
        "id": "GuBFF1rmbULR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Model \"\"\"\n",
        "model = build_unet(input_shape,NUM_CLASSES)\n",
        "model.compile(\n",
        "    loss = \"categorical_crossentropy\",\n",
        "    optimizer = Adam(lr),\n",
        ")"
      ],
      "metadata": {
        "id": "BthLfbCRN3Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Training \"\"\"\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(model_path,verbose=1,save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "    CSVLogger(csv_path,append=True),\n",
        "    EarlyStopping(monitor='val_loss',patience=20,restore_best_weights=False)\n",
        "]"
      ],
      "metadata": {
        "id": "mipuJ8tDOTBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset,validation_data=valid_dataset,epochs=epochs,callbacks=callbacks)"
      ],
      "metadata": {
        "id": "gVDInIF1QEJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}